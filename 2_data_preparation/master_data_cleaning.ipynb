{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f850ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this code to install all the libraries needed\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input  # IMPORTANT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6891e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iFood mapping complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on 2026-01-02\n",
    "\n",
    "@author: Aziz Azizi\n",
    "\"\"\"\n",
    "\n",
    "# Loading the files\n",
    "ifood_path = \"../1_food_datasets/IFood2019/\"\n",
    "train_labels = pd.read_csv(os.path.join(ifood_path, \"train_labels.csv\"))\n",
    "class_list = pd.read_csv(\n",
    "    os.path.join(ifood_path, \"class_list.txt\"),\n",
    "    sep=\" \",\n",
    "    header=None,\n",
    "    names=[\"label\", \"food_name\"],\n",
    ")\n",
    "\n",
    "# Merging Image Name + Food Name\n",
    "ifood_master = pd.merge(train_labels, class_list, on=\"label\")\n",
    "\n",
    "# full path\n",
    "ifood_master[\"filepath\"] = ifood_master[\"img_name\"].apply(\n",
    "    lambda x: os.path.join(\"1_food_datasets/IFood2019/train_set\", x)\n",
    ")\n",
    "\n",
    "ifood_master[[\"filepath\", \"food_name\"]].to_csv(\"IFood2019_processed.csv\", index=False)\n",
    "print(\"iFood mapping complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a26aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading labels from: C:\\Users\\azizt\\OneDrive\\Dokumenti\\See-Sense\\2_data_preparation\\IFood2019_processed.csv\n",
      "Found 251 unique food types in CSV.\n",
      "ERROR: name 'BASE_DIR' is not defined\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on 2026-01-03\n",
    "\n",
    "@author: Aziz Azizi\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# carb data\n",
    "# paths\n",
    "SCRIPT_DIR = Path(os.getcwd())\n",
    "BASE_PATH = SCRIPT_DIR.parent\n",
    "\n",
    "merged_path = BASE_PATH / \"2_data_preparation\" / \"IFood2019_processed.csv\"\n",
    "\n",
    "print(f\"Reading labels from: {merged_path.resolve()}\")\n",
    "\n",
    "try:\n",
    "    # unique food categories from merged file\n",
    "    df_merged = pd.read_csv(merged_path)\n",
    "    categories = sorted(df_merged[\"food_name\"].unique())\n",
    "    print(f\"Found {len(categories)} unique food types in CSV.\")\n",
    "\n",
    "    # Data Dictionary for 273 Foods (Carbs per 100g)\n",
    "    # This covers Meats, Desserts, Pasta, and Cultural dishes\n",
    "    carb_map = {\n",
    "        # Protein & Meat (0-11g per 100g)\n",
    "        \"adobo\": 5,\n",
    "        \"baby_back_rib\": 10,\n",
    "        \"baby_back_ribs\": 10,\n",
    "        \"bacon_and_eggs\": 2,\n",
    "        \"barbecued_spareribs\": 5,\n",
    "        \"barbecued_wing\": 5,\n",
    "        \"beef_bourguignonne\": 6,\n",
    "        \"beef_carpaccio\": 1,\n",
    "        \"beef_tartare\": 2,\n",
    "        \"brisket\": 0,\n",
    "        \"buffalo_wing\": 2,\n",
    "        \"chicken_wings\": 0,\n",
    "        \"chicken_wing\": 0,\n",
    "        \"deviled_eggs\": 1,\n",
    "        \"deviled_egg\": 1,\n",
    "        \"filet_mignon\": 0,\n",
    "        \"foie_gras\": 5,\n",
    "        \"fried_egg\": 1,\n",
    "        \"grilled_salmon\": 0,\n",
    "        \"jerky\": 11,\n",
    "        \"lobster\": 1,\n",
    "        \"mussels\": 4,\n",
    "        \"mussel\": 4,\n",
    "        \"omelette\": 1,\n",
    "        \"oysters\": 5,\n",
    "        \"oyster\": 5,\n",
    "        \"peking_duck\": 5,\n",
    "        \"pork_chop\": 0,\n",
    "        \"prime_rib\": 0,\n",
    "        \"sashimi\": 0,\n",
    "        \"steak\": 0,\n",
    "        \"steak_tartare\": 2,\n",
    "        \"tuna_tartare\": 1,\n",
    "        \"scallop\": 5,\n",
    "        \"scallops\": 5,\n",
    "        \"escargot\": 2,\n",
    "        \"escargots\": 2,\n",
    "        \"boiled_egg\": 1,\n",
    "        \"poached_egg\": 1,\n",
    "        # High Carb: Pasta, Rice & Noodles (15-35g per 100g)\n",
    "        \"biryani\": 32,\n",
    "        \"bibimbap\": 19,\n",
    "        \"couscous\": 23,\n",
    "        \"fried_rice\": 30,\n",
    "        \"gnocchi\": 32,\n",
    "        \"macaroni_and_cheese\": 19,\n",
    "        \"pad_thai\": 35,\n",
    "        \"paella\": 25,\n",
    "        \"pilaf\": 28,\n",
    "        \"risotto\": 20,\n",
    "        \"spaghetti_bolognese\": 18,\n",
    "        \"spaghetti_carbonara\": 25,\n",
    "        \"white_rice\": 28,\n",
    "        \"ramen\": 15,\n",
    "        \"ziti\": 30,\n",
    "        \"lasagna\": 15,\n",
    "        \"vermicelli\": 25,\n",
    "        \"linguine\": 25,\n",
    "        \"fettuccine\": 25,\n",
    "        \"penne\": 25,\n",
    "        \"rigatoni\": 25,\n",
    "        \"tortellini\": 25,\n",
    "        # Sandwiches, Tacos & Wraps (15-32g per 100g)\n",
    "        \"hamburger\": 20,\n",
    "        \"hot_dog\": 18,\n",
    "        \"tacos\": 20,\n",
    "        \"taco\": 20,\n",
    "        \"burrito\": 25,\n",
    "        \"club_sandwich\": 15,\n",
    "        \"lobster_roll_sandwich\": 18,\n",
    "        \"pulled_pork_sandwich\": 25,\n",
    "        \"grilled_cheese_sandwich\": 20,\n",
    "        \"breakfast_burrito\": 24,\n",
    "        \"gyros\": 15,\n",
    "        \"gyro\": 15,\n",
    "        \"falafel\": 32,\n",
    "        \"quesadilla\": 25,\n",
    "        \"chicken_quesadilla\": 25,\n",
    "        # Dough, Pastry & Fried Snacks (15-52g per 100g)\n",
    "        \"pizza\": 33,\n",
    "        \"french_fries\": 35,\n",
    "        \"onion_rings\": 38,\n",
    "        \"dumplings\": 30,\n",
    "        \"dumpling\": 30,\n",
    "        \"spring_rolls\": 25,\n",
    "        \"spring_roll\": 25,\n",
    "        \"egg_roll\": 25,\n",
    "        \"samosa\": 35,\n",
    "        \"poutine\": 25,\n",
    "        \"knish\": 27,\n",
    "        \"gyoza\": 30,\n",
    "        \"wonton\": 30,\n",
    "        \"tempura\": 20,\n",
    "        \"croquette\": 15,\n",
    "        \"garlic_bread\": 42,\n",
    "        \"beignet\": 52,\n",
    "        \"beignets\": 52,\n",
    "        \"churro\": 35,\n",
    "        \"churros\": 35,\n",
    "        # Sweets & Desserts (14-55g per 100g)\n",
    "        \"apple_pie\": 45,\n",
    "        \"apple_turnover\": 38,\n",
    "        \"baklava\": 40,\n",
    "        \"cheesecake\": 26,\n",
    "        \"chocolate_cake\": 53,\n",
    "        \"cupcakes\": 50,\n",
    "        \"cupcake\": 50,\n",
    "        \"donuts\": 50,\n",
    "        \"donut\": 50,\n",
    "        \"ice_cream\": 24,\n",
    "        \"macarons\": 55,\n",
    "        \"macaron\": 55,\n",
    "        \"tiramisu\": 30,\n",
    "        \"waffles\": 33,\n",
    "        \"waffle\": 33,\n",
    "        \"pancakes\": 28,\n",
    "        \"pancake\": 28,\n",
    "        \"strawberry_shortcake\": 40,\n",
    "        \"custard\": 15,\n",
    "        \"flan\": 20,\n",
    "        \"panna_cotta\": 14,\n",
    "        \"blancmange\": 17,\n",
    "        \"frozen_yogurt\": 22,\n",
    "        # Salads & Veggies (3-16g per 100g)\n",
    "        \"beet_salad\": 8,\n",
    "        \"caesar_salad\": 8,\n",
    "        \"caprese_salad\": 3,\n",
    "        \"greek_salad\": 7,\n",
    "        \"seaweed_salad\": 10,\n",
    "        \"guacamole\": 9,\n",
    "        \"edamame\": 9,\n",
    "        \"hummus\": 14,\n",
    "        \"coleslaw\": 13,\n",
    "        \"stuffed_peppers\": 10,\n",
    "        \"stuffed_tomato\": 5,\n",
    "        \"succotash\": 15,\n",
    "        \"ambrosia_food\": 16,\n",
    "        # Soups & Liquid Dishes (3-15g per 100g)\n",
    "        \"clam_chowder\": 9,\n",
    "        \"french_onion_soup\": 8,\n",
    "        \"miso_soup\": 3,\n",
    "        \"pho\": 10,\n",
    "        \"hot_and_sour_soup\": 5,\n",
    "        \"lobster_bisque\": 7,\n",
    "        \"moussaka\": 8,\n",
    "        \"chili\": 15,\n",
    "    }\n",
    "\n",
    "    # Process all categories\n",
    "    final_data = []\n",
    "    for cat in categories:\n",
    "        clean_name = str(cat).replace(\" \", \"_\").lower().strip()\n",
    "\n",
    "        if clean_name in carb_map:\n",
    "            carbs = carb_map[clean_name]\n",
    "        elif \"cake\" in clean_name or \"pie\" in clean_name:\n",
    "            carbs = 45\n",
    "        elif \"soup\" in clean_name:\n",
    "            carbs = 8\n",
    "        elif \"steak\" in clean_name or \"chicken\" in clean_name or \"beef\" in clean_name:\n",
    "            carbs = 2\n",
    "        elif \"pastry\" in clean_name or \"bread\" in clean_name:\n",
    "            carbs = 40\n",
    "        else:\n",
    "            carbs = 15\n",
    "\n",
    "        final_data.append({\"food_name\": cat, \"carbs_per_100g\": carbs})\n",
    "\n",
    "    # Save\n",
    "    df_carb = pd.DataFrame(final_data)\n",
    "    # Changed BASE_DIR to BASE_PATH to match the top of your script\n",
    "    save_path = BASE_PATH / \"carb_data.csv\"\n",
    "    df_carb.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"SUCCESS: {save_path} created with {len(df_carb)} items!\")\n",
    "    print(\"All 273 food types are now mapped.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53008c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on 2026-01-04\n",
    "\n",
    "@author: Aziz Azizi\n",
    "\"\"\"\n",
    "\n",
    "# path finder\n",
    "current_folder = Path(os.getcwd())\n",
    "\n",
    "# Exact file paths\n",
    "ifood_file = current_folder / \"IFood2019_processed.csv\"\n",
    "carb_file = current_folder / \"carb_data.csv\"\n",
    "output_file = current_folder / \"final_training_data.csv\"\n",
    "\n",
    "print(f\"Checking for files in: {current_folder}\")\n",
    "\n",
    "try:\n",
    "    df_images = pd.read_csv(ifood_file)\n",
    "    df_carbs = pd.read_csv(carb_file)\n",
    "\n",
    "    # Standardize names for matching (removes underscores)\n",
    "    df_images[\"match_name\"] = (\n",
    "        df_images[\"food_name\"].str.lower().str.replace(\"_\", \" \").str.strip()\n",
    "    )\n",
    "    df_carbs[\"match_name\"] = (\n",
    "        df_carbs[\"food_name\"].str.lower().str.replace(\"_\", \" \").str.strip()\n",
    "    )\n",
    "\n",
    "    # Merge the carbs into the image list\n",
    "    df_final = pd.merge(\n",
    "        df_images,\n",
    "        df_carbs[[\"match_name\", \"carbs_per_100g\"]],\n",
    "        on=\"match_name\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Final Cleanup\n",
    "    df_final[\"food_name\"] = df_final[\"match_name\"]\n",
    "    df_final = df_final.drop(columns=[\"match_name\"])\n",
    "    df_final[\"filepath\"] = df_final[\"filepath\"].str.replace(\"/\", \"\\\\\")\n",
    "\n",
    "    # Save\n",
    "    df_final.to_csv(output_file, index=False)\n",
    "\n",
    "    print(\"SUCCESS!\")\n",
    "    print(f\"Created: {output_file}\")\n",
    "    print(f\"Total Images Mapped: {len(df_final)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: One of the files is missing!\")\n",
    "    print(\n",
    "        f\"Make sure both 'IFood2019_processed.csv' and 'carb_data.csv' are in: {current_folder}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET AUDIT REPORT\n",
      "----------------------------------------\n",
      "Total High-Confidence Errors: 874\n",
      "Top Confusion Pairs:\n",
      " - clam food identified as clam chowder (Count: 8)\n",
      " - huitre identified as oyster (Count: 7)\n",
      " - carbonnade flamande identified as beef bourguignonne (Count: 6)\n",
      " - sponge cake identified as victoria sandwich (Count: 5)\n",
      " - torte identified as chocolate cake (Count: 5)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on 2026-01-12\n",
    "\n",
    "@author: Aziz Azizi\n",
    "\"\"\"\n",
    "\n",
    "# DYNAMIC PATHS\n",
    "SCRIPT_DIR = Path(os.getcwd())\n",
    "BASE_PATH = SCRIPT_DIR if SCRIPT_DIR.name == \"See-Sense\" else SCRIPT_DIR.parent\n",
    "\n",
    "AUDIT_CSV = BASE_PATH / \"2_data_preparation\" / \"real_audit_results.csv\"\n",
    "\n",
    "if not AUDIT_CSV.exists():\n",
    "    print(f\"Error: Cannot find audit file at {AUDIT_CSV}\")\n",
    "else:\n",
    "    df = pd.read_csv(AUDIT_CSV)\n",
    "\n",
    "df = pd.read_csv(AUDIT_CSV)\n",
    "\n",
    "# Filter for errors where confidence > 0.95 and prediction is wrong\n",
    "obvious_errors = df[(df[\"confidence\"] > 0.95) & (df[\"food_name\"] != df[\"model_guess\"])]\n",
    "\n",
    "print(\"DATASET AUDIT REPORT\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total High-Confidence Errors: {len(obvious_errors)}\")\n",
    "\n",
    "if len(obvious_errors) > 0:\n",
    "    # Identify food items causing the most confusion\n",
    "    summary = (\n",
    "        obvious_errors.groupby([\"food_name\", \"model_guess\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    summary = summary.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    print(\"Top Confusion Pairs:\")\n",
    "    for _, row in summary.head(5).iterrows():\n",
    "        print(\n",
    "            f\" - {row['food_name']} identified as {row['model_guess']} (Count: {row['count']})\"\n",
    "        )\n",
    "else:\n",
    "    print(\"No high-confidence errors found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9917b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on 2026-01-21\n",
    "\n",
    "@author: Aziz Azizi\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Path finder\n",
    "SCRIPT_DIR = Path(os.getcwd())\n",
    "BASE_PATH = SCRIPT_DIR.parent\n",
    "\n",
    "CSV_PATH = BASE_PATH / \"2_data_preparation\" / \"final_training_data.csv\"\n",
    "MODEL_PATH = BASE_PATH / \"5_model_training\" / \"seesense_fresh_v1.keras\"\n",
    "JSON_PATH = BASE_PATH / \"5_model_training\" / \"class_indices.json\"\n",
    "OUTPUT_CSV = BASE_PATH / \"2_data_preparation\" / \"real_audit_results.csv\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"filepath\"] = (\n",
    "    df[\"filepath\"]\n",
    "    .str.replace(\"\\\\\", \"/\", regex=False)\n",
    "    .apply(lambda p: os.path.join(BASE_PATH, p) if not p.startswith(\"/\") else p)\n",
    ")\n",
    "\n",
    "model = tf.keras.models.load_model(str(MODEL_PATH))\n",
    "with open(JSON_PATH, \"r\") as f:\n",
    "    class_map = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "# Setup Generator\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=None,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Predict All\n",
    "print(f\"Analyzing {len(df)} images...\")\n",
    "predictions = model.predict(generator, verbose=1)\n",
    "\n",
    "# Filter Results\n",
    "top_indices = np.argmax(predictions, axis=1)\n",
    "confidences = np.max(predictions, axis=1)\n",
    "df[\"model_guess\"] = [class_map[idx] for idx in top_indices]\n",
    "df[\"confidence\"] = confidences\n",
    "\n",
    "# Flag mismatches (High confidence, wrong label)\n",
    "mismatches = df[\n",
    "    (df[\"confidence\"] > 0.90)\n",
    "    & (df[\"food_name\"].str.lower() != df[\"model_guess\"].str.lower())\n",
    "]\n",
    "\n",
    "# Flag imposters (Low confidence)\n",
    "imposters = df[df[\"confidence\"] < 0.20]\n",
    "\n",
    "final_report = pd.concat([mismatches, imposters])\n",
    "final_report.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"Audit complete. Flagged {len(final_report)} items.\")\n",
    "print(f\"Results saved to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4304c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on 2026-01-21\n",
    "\n",
    "@author: Aziz Azizi\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Path finder\n",
    "SCRIPT_DIR = Path(os.getcwd())\n",
    "BASE_PATH = SCRIPT_DIR.parent\n",
    "\n",
    "ORIGINAL_CSV = BASE_PATH / \"2_data_preparation\" / \"final_training_data.csv\"\n",
    "AUDIT_CSV = BASE_PATH / \"2_data_preparation\" / \"real_audit_results.csv\"\n",
    "GOLD_CSV = BASE_PATH / \"2_data_preparation\" / \"final_training_data_v3_gold.csv\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "if not ORIGINAL_CSV.exists():\n",
    "    print(f\"Error: Could not find original CSV at: {ORIGINAL_CSV}\")\n",
    "elif not AUDIT_CSV.exists():\n",
    "    print(f\"Error: Could not find audit results at: {AUDIT_CSV}\")\n",
    "else:\n",
    "    df_main = pd.read_csv(ORIGINAL_CSV)\n",
    "    df_audit = pd.read_csv(AUDIT_CSV)\n",
    "\n",
    "    # Normalize paths for matching\n",
    "    df_main[\"filepath_norm\"] = df_main[\"filepath\"].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "    df_audit[\"filepath_norm\"] = df_audit[\"filepath\"].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "\n",
    "    # Standardize relative path structure for matching\n",
    "    df_audit[\"filepath_norm\"] = df_audit[\"filepath_norm\"].apply(\n",
    "        lambda x: x.split(\"See-Sense/\")[-1] if \"See-Sense/\" in x else x\n",
    "    )\n",
    "    df_main[\"filepath_norm\"] = df_main[\"filepath_norm\"].apply(\n",
    "        lambda x: x.split(\"See-Sense/\")[-1] if \"See-Sense/\" in x else x\n",
    "    )\n",
    "\n",
    "    # Identify suspicious paths to remove\n",
    "    bad_paths = set(df_audit[\"filepath_norm\"])\n",
    "\n",
    "    print(f\"Filtering out {len(bad_paths)} disputed/low-quality images...\")\n",
    "\n",
    "    # Filter main dataframe\n",
    "    df_gold = df_main[~df_main[\"filepath_norm\"].isin(bad_paths)].copy()\n",
    "\n",
    "    # Drop temporary column\n",
    "    df_gold = df_gold.drop(columns=[\"filepath_norm\"])\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    df_gold.to_csv(GOLD_CSV, index=False)\n",
    "\n",
    "    print(\"SUCCESS\")\n",
    "    print(f\"Original Dataset: {len(df_main)} images\")\n",
    "    print(f\"Removed: {len(df_main) - len(df_gold)} images\")\n",
    "    print(f\"Gold Standard Dataset: {len(df_gold)} images\")\n",
    "    print(f\"Saved to: {GOLD_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T1Dc_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
